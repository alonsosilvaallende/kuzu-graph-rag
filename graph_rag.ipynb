{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph RAG\n",
    "\n",
    "\"Graph RAG\" has become an incredible buzz term of late. The goal of this notebook is to provide a simple and intuitive demonstration of what Graph RAG accomplishes, and how to use two open source, embedded databases (Kùzu, for graph traversal, and LanceDB, for vector search) to combine the benefits of vector and graph databases.\n",
    "\n",
    "## What is Graph RAG?\n",
    "\n",
    "At its core, Graph RAG aims to combine the power of knowledge graphs with the well-known benefits of vector (semantic) search. Knowledge graphs are great for storing and traversing relationships between entities, while vector embeddings are great for capturing the semantic similarity between chunks of data. By combining the two, we can create a powerful system that can answer complex queries that require both semantic similarity and relationship traversal.\n",
    "\n",
    "## How and why does Graph RAG work in practice?\n",
    "\n",
    "It's worth going over how and why Graph RAG makes sense, intuitively. Semantic search based on vector similarity leverages the _implicit_ relationships between entities - two vector embeddings that represent different chunks of text may be close to each other in vector space, indicating that they are semantically similar. On the other hand, knowledge graphs store _explicit_ relationships between entities - two nodes in a graph may be connected by an explicit relationship (termed an \"edge\"), indicating that they are related.\n",
    "\n",
    "By combining the two, we can create a system that can answer complex queries that require both semantic similarity and relationship traversal. The code in this notebook demonstrates this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Graph-only retrieval\n",
    "\n",
    "First, let's demonstrate how to extract information into a knowledge graph and store it in [Kùzu](https://kuzudb.com/), an open source, embedded graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from typing import List, Literal, Optional\n",
    "from llama_index.core import PropertyGraphIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "from llama_index.graph_stores.kuzu import KuzuPropertyGraphStore\n",
    "\n",
    "import kuzu\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = (\n",
    "    \"openai_api_key\"\n",
    ")\n",
    "\n",
    "shutil.rmtree(\"test_kuzudb\", ignore_errors=True)\n",
    "db = kuzu.Database(\"test_kuzudb\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the embedding model and LLM\n",
    "embed_model = OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
    "extraction_llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "generation_llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data/curie/\").load_data()\n",
    "\n",
    "entities = Literal[\"PERSON\", \"NOBEL_PRIZE\", \"LOCATION\", \"DISCOVERY\"]\n",
    "relations = Literal[\"DISCOVERED\", \"IS_MARRIED_TO\", \"WORKED_WITH\", \"WON\"]\n",
    "\n",
    "# Define explicit relationship directions as a list of triples\n",
    "# The graph extraction process will be guided by this \"schema\"\n",
    "validation_schema = [\n",
    "    (\"PERSON\", \"IS_MARRIED_TO\", \"PERSON\"),\n",
    "    (\"PERSON\", \"WORKED_WITH\", \"PERSON\"),\n",
    "    (\"PERSON\", \"WON\", \"NOBEL_PRIZE\"),\n",
    "    (\"PERSON\", \"DISCOVERED\", \"DISCOVERY\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = KuzuPropertyGraphStore(\n",
    "    db,\n",
    "    has_structured_schema=True,\n",
    "    relationship_schema=validation_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_path_extractor = SchemaLLMPathExtractor(\n",
    "    llm=extraction_llm,\n",
    "    possible_entities=entities,\n",
    "    possible_relations=relations,\n",
    "    kg_validation_schema=validation_schema,\n",
    "    strict=True,  # if false, will allow triples outside of the schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 550.94it/s]\n",
      "Extracting paths from text with schema: 100%|██████████| 1/1 [00:02<00:00,  2.85s/it]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  3.73it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set up the property graph index\n",
    "kg_index = PropertyGraphIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embed_model,\n",
    "    kg_extractors=[schema_path_extractor],\n",
    "    property_graph_store=graph_store,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the graph is created, we can explore it in [Kùzu Explorer](https://github.com/kuzudb/explorer), a web-base UI, by running a Docker container that pulls the latest image of Kùzu Explorer as follows:\n",
    "\n",
    "```bash\n",
    "docker run -p 8000:8000 \\\n",
    "           -v ./test_kuzudb:/database \\\n",
    "           -e MODE=READ_ONLY \\\n",
    "           --rm kuzudb/explorer:latest\n",
    "```\n",
    "\n",
    "Then, launch the UI and then visting http://localhost:8000/.\n",
    "\n",
    "The easiest way to see the entire graph is to use a Cypher query like `MATCH (a)-[b]->(c) RETURN * LIMIT 100`.\n",
    "\n",
    "For this dataset, the graph constructed looks as follows:\n",
    "\n",
    "![](./assets/kuzu_graph_rag.png)\n",
    "\n",
    "The dataset is about the scientist Marie Curie and her discoveries, as well as her direct and indirect relationships to persons like Pierre Curie, Paul Langevin and Albert Einstein. The graph has an explicit schema, specified by us, and captures entities from the unstructured data like \"Polonium\", \"Radium\", and \"Nobel Prize in Physics\", etc., and edges representing relationships between these entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of graph quality\n",
    "\n",
    "Graph construction is a critical step in the process of building a Graph RAG system. The quality of the graph will directly impact the quality of the results. In this notebook, we will use a simple example that leverages an LLM to demonstrate the idea. In practice, you would use more sophisticated methods to construct a knowledge graph, such as custom ML models or APIs (Rebel, GliNER/GliREL, DiffBot, WhyHow Knowledge Graph Studio, etc.).\n",
    "\n",
    "The key is to _persist_ the graph in a graph database, so that it can be managed and queried efficiently. Kùzu is a great choice for this purpose, as it is an open source, embedded graph database that is easy to use and deploy.\n",
    "\n",
    "The LLM-generated graph can be incomplete, noisy, or contain errors. It is important to clean and refine the graph before storing it in the database. This process is called \"graph curation\" and is essential for the quality of the results. The following cell performs the task of explicitly defining the graph and storing specific nodes and relationships in the already-existing knowledge graph, and persists it to the Kùzu database that sits on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.graph_stores.types import Relation, EntityNode\n",
    "\n",
    "# graph_store.upsert_nodes(\n",
    "#     [\n",
    "#         EntityNode(label=\"PERSON\", name=\"Jacques Curie\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# graph_store.upsert_relations(\n",
    "#     [\n",
    "#         Relation(\n",
    "#             label=\"WORKED_WITH\",\n",
    "#             source_id=\"Pierre Curie\",\n",
    "#             target_id=\"Paul Langevin\",\n",
    "#         ),\n",
    "#         Relation(\n",
    "#             label=\"DISCOVERED\",\n",
    "#             source_id=\"Jacques Curie\",\n",
    "#             target_id=\"piezoelectricity\",\n",
    "#         ),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierre Curie and Jacques Curie discovered piezoelectricity.\n"
     ]
    }
   ],
   "source": [
    "kg_retriever = kg_index.as_retriever()\n",
    "kg_query_engine = kg_index.as_query_engine(include_text=False)\n",
    "\n",
    "response = kg_query_engine.query(\"Who discovered Piezoelectricity?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierre Curie worked with Marie Curie and Paul Langevin.\n"
     ]
    }
   ],
   "source": [
    "response = kg_query_engine.query(\"Who did Pierre Curie work with?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways from graph-only retrieval\n",
    "\n",
    "It can be seen by inspecting the raw data that the LLM-extracted graph is incomplete. Once the right nodes/relationships are added to the graph, the quality of the graph-based retrieval improves significantly. This did require some manual curation, but we will demonstrate below that this process is worth it, by trying to answer the **same** questions using vector-only retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector-only retrieval\n",
    "\n",
    "This stage demonstrates how to extract information into a vector database and store it in [LanceDB](https://lancedb.com/), an open source, embedded vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use LanceDB to perform vector similarity search\n",
    "shutil.rmtree(\"./test_lancedb\", ignore_errors=True)\n",
    "\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"openai_api_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-15T22:06:27Z WARN  lance::dataset] No existing dataset at /Users/prrao/code/kuzu-graph-rag/test_lancedb/vectors.lance, it will be created\n"
     ]
    }
   ],
   "source": [
    "vector_store = LanceDBVectorStore(\n",
    "    uri=\"./test_lancedb\",\n",
    "    mode=\"overwrite\",\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "    llm=OpenAI(model=\"gpt-4o-mini\", temperature=0.1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pierre Curie discovered piezoelectricity.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_retriever = vector_index.as_retriever(similarity_top_k=4)\n",
    "vector_query_engine = RetrieverQueryEngine(vector_retriever)\n",
    "\n",
    "response = vector_query_engine.query(\"Who discovered Piezoelectricty?\")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pierre Curie worked with his brother Jacques in discovering piezoelectricity.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = vector_query_engine.query(\"Who did Pierre Curie work with?\")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways from vector-only retrieval\n",
    "\n",
    "Due to the nature of the data and the questions being asked, the vector-only retrieval obtains _partial_ answers to the questions. This is because the vector embeddings are not able to capture the deeper relationships between the entities in the text. This is where the graph-based retrieval provides value, as it can capture these relationships and provide more accurate answers.\n",
    "\n",
    "However, this is **not** to say that graph retrieval is better than vector retrieval - in many cases, semantic similarity can help narrow down the search space and provide useful insights. The aim of Graph RAG is to combine the two methods, which we will demonstrate in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Combining graph and vector retrieval to build a Graph RAG system\n",
    "\n",
    "In this stage, we will demonstrate how to combine graph and vector retrieval and rerank the results in order to provide better context to the LLM prior to generating the response. We will use the afore-mentioned Kùzu and LanceDB databases to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.postprocessor.cohere_rerank.base import CohereRerank\n",
    "\n",
    "\n",
    "class CustomRerankerRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever with cohere reranking.\"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            kg_retriever,\n",
    "            vector_retriever,\n",
    "            cohere_api_key: Optional[str] = None,\n",
    "            cohere_top_n: int = 2,\n",
    "        ):\n",
    "        self._kg_retriever = kg_retriever\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._reranker = CohereRerank(\n",
    "            api_key=cohere_api_key, top_n=cohere_top_n\n",
    "        )\n",
    "\n",
    "    def _retrieve(self, query: str) -> List[NodeWithScore]:\n",
    "        \"\"\"Define custom retriever with reranking.\n",
    "\n",
    "        Could return `str`, `TextNode`, `NodeWithScore`, or a list of those.\n",
    "        \"\"\"\n",
    "        vector_retrieval_nodes = self._vector_retriever.retrieve(query)\n",
    "        kg_retrieval_nodes = self._kg_retriever.retrieve(query)\n",
    "        combined_nodes = vector_retrieval_nodes + kg_retrieval_nodes\n",
    "        reranked_nodes = self._reranker.postprocess_nodes(\n",
    "            combined_nodes,\n",
    "            query_str=str(query),\n",
    "        )\n",
    "        unique_nodes = {n.node_id: n for n in reranked_nodes}\n",
    "        return list(unique_nodes.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_reranker_retriever = CustomRerankerRetriever(\n",
    "    kg_retriever,\n",
    "    vector_retriever,\n",
    "    cohere_api_key=\"cohere_api_key\",\n",
    "    cohere_top_n=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Langevin and Marie Curie\n"
     ]
    }
   ],
   "source": [
    "custom_reranker_query_engine = RetrieverQueryEngine(custom_reranker_retriever)\n",
    "\n",
    "response = custom_reranker_query_engine.query(\"Who did Pierre Curie work with?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways from a Graph RAG perspective\n",
    "\n",
    "As can be seen from the above example, combining graph and vector retrieval can provide more accurate and contextually relevant answers to the questions. This is because the graph-based retrieval can capture the relationships between the entities in the text, while the vector-based retrieval can provide semantic similarity between the entities. By combining the two, we can build a powerful system that can answer complex queries that require both semantic similarity and relationship traversal.\n",
    "\n",
    "In practice, the Graph RAG system can be used to answer a wide range of questions, such as factoid questions, definition questions, and reasoning questions. The key is to build a high-quality knowledge graph, and to combine it with vector search in a way that provides the most relevant and accurate answers to the questions.\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "Graph RAG can be thought of as a suite of methodologies that combine the power of knowledge graphs with the benefits of vector search. Databases like Kùzu and LanceDB, due to their ease of use, developer friendliness and permissive licensing, are great choices for building a Graph RAG system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kuzu-graph-rag",
   "language": "python",
   "name": "kuzu-graph-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
